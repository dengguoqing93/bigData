```text
echo "echo never > /sys/kernel/mm/transparent_hugepage/enabled" >> /etc/rc.local
echo "echo never > /sys/kernel/mm/transparent_hugepage/defrag" >> /etc/rc.local
```

## 截止kafka的学习的软件的集群搭建

​		两个月左右的学习时间，快速学习了很多软件的使用及原理，但是没有真正搭建过完整的集群。现在是一个阶段的结束，要有一个实战项目了，趁着这个空，搭建一下高可用集群。

​		学习的顺序是按照hadoop->hive->hue->hbase->zookeeper->redis->kafka，但是高可用的集群需要依赖zookeeper,而且kafka和redis与hadoop体系是相对独立的，所以这里先搭建zookeeper+kafka+redis。然后搭建hadoop的体系系统。

​		这里选用的zookeeper版本为：3.5.8、kafka的版本为:kafka_2.12-2.4.1,选择redis的版本为： redis-5.0.5





#### zookeeper的集群搭建

1. 解压zookeeper压缩包

2. 创建zookeeper需要的目录

   ```shell
   ##创建zookeeper保存数据的目录和日志目录
   mkdir /opt/zookeeper/data
   mkdir /opt/zookeeper/data/logs
   ```

3. 复制并修改zookeepr配置文件

   ```shell
    #复制配置文件
    cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg 
   ```

4. 修改配置文件

   ```conf
   #更新datadir
   dataDir=/opt/zookeeper/data
   #增加logdir
   dataLogDir=/opt/zookeeper/data/logs
   #增加集群配置
   ##server.服务器ID=服务器IP地址：服务器之间通信端⼝：服务器之间投票选举端⼝
   server.1=master:2888:3888
   server.2=slave0:2888:3888
   server.3=slave2:2888:3888
   #打开注释
   #ZK提供了⾃动清理事务⽇志和快照⽂件的功能，这个参数指定了清理频率，单位是⼩时
   autopurge.purgeInterval=1
   ```

   

5. 添加myid配置

   在zookeeper的data目录下创建一个myid文件，内容为1，该文件是记录每个服务器的ID

   ```shell
   cd /opt/zookeeper/data
   echo 1 > myid		
   ```

6. 分发安装包

   ```shell
   scp -r zookeeper slave0:/opt/
   scp -r zookeeper slave2:/opt/
   ##或
   rsync -av zookeeper slave0:/opt/
   rsync -av zookeeper slave2:/opt/
   ```

7. 修改slave0和slave2的myid值

   ```shell
   ##slave0
   echo 2 > /opt/zookeeper/data/myid
   ## slave2
   echo 3 > /opt/zookeeper/data/myid	
   ```

8. 修改环境变量

   ```shell
   export JAVA_HOME=/opt/jdk8
   export KAFKA_HOME=/opt/kafka
   export ZOOKEEPER_HOME=/opt/zookeeper
   export PATH=$PATH:$JAVA_HOME/bin:$KAFKA_HOME/bin:$ZOOKEEPER_HOME/bin
   ```

   

9. 使用如下脚本启动zookeeper

   ```shell
   #!/bin/sh
   echo "start zookeeper server..."
   if(($#==0));then
   echo "no params";
   exit;
   fi
   hosts="master slave0 slave2"
   for host in $hosts
   do
   ssh $host "source /etc/profile; /opt/zookeeper/bin/zkServer.sh $1"
   done
   ```

   ```shell
   ./zk.sh start		
   ```

10. 使用如下命令查看启动状态

    ```shell
    ./zk.sh status
    ```

    如下图启动成功

![image-20200824101548760](/Users/dengguoqing/IdeaProjects/lagou/image-20200824101548760.png)

#### kafka集群搭建

1. 解压kafka包

   ```shell
   tar /opt/kafka_2.12-2.4.1.tgz
   ```

   

2. 修改配置文件，并使其生效

   ```shell
   # 配置环境变量，三台Linux都要配置
   vim /etc/profile
   # 添加以下内容：
   export KAFKA_HOME=/opt/kafka_2.12-1.0.2
   export PATH=$PATH:$KAFKA_HOME/bin
   
   source /etc/profile
   ```

   

3. 修改kafka配置文件server.properties

   ```shell
   vim /opt/kafka/config/server.properties
   broker.id=0
   listeners=PLAINTEXT://:9092
   ##其他机器，修改机器名
   advertised.listeners=PLAINTEXT://master:9092
   log.dirs=/var/kafka/kafka-logs
   zookeeper.connect=master:2181,slave0:2181,slave2:2181/kafka
   ```

4. 通过启动命令，启动kafka

   ```shell
   kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
   ```
   
5. 启动脚本

   ```shell
   #!/bin/sh
   echo "start kafka server..."
   hosts="master slave0 slave2"
   for host in $hosts
   do
   ssh $host "source /etc/profile; kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties"
   done
   ```

   





#### hadoop集群搭建

1. 解压hadoop，这里hadoop版本为2.9.2

   ```shell
   tar -zxvf hadoop-2.9.2.tar.gz
   ```

2. ##### hdfs集群配置

   1. 修改hadoop-env.sh

      ```
      vim hadoop-env.sh	
      export JAVA_HOME=/opt/java8
      ```

   2. 指定namenode节点以及数据存储目录

      ```xml
      vim core-site.xml
      <!-- 指定HDFS中NameNode的地址 -->
      <property>
      <name>fs.defaultFS</name>
      <value>hdfs://master:9000</value>
      </property>
      <!-- 指定Hadoop运⾏时产⽣⽂件的存储⽬录 -->
      <property>
      <name>hadoop.tmp.dir</name>
      <value>/opt/hadoop-2.9.2/data</value>
      </property>
      ```

   3. 指定secondaryNamenode节点

      ```shell
      vim hdfs-site.xml	
      ```

      ```xml
      <!-- 指定Hadoop辅助名称节点主机配置 -->
      <property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>slave0:50090</value>
      </property>
      <!--副本数量 -->
      <property>
      <name>dfs.replication</name>
      <value>3</value>
      </property>	
      ```

   4. 指定datanode从节点

      ```shell
      vim slaves
      
      master
      slave0
      slave2
      ```

3. ##### mapReduce集群配置

   1. 指定MapReduce使⽤的jdk路径（修改mapred-env.sh）

      ```shell
      export JAVA_HOME=/opt/jdk8
      ```
   
2. 指定MapReduce计算框架运⾏Yarn资源调度框架(修改mapred-site.xml)
  
   ```shell
      cp mapred-site.xml.template mapred-site.xml
      vim mapred-site.xml
   ```
   
   ```xml
      <!-- 指定MR运⾏在Yarn上 -->
      <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
      </property>
   ```
   
4. ##### YARN集群配置

   1. 指定JDK路径

      ```shell
      vim yarn-env.sh
      export JAVA_HOME=/opt/java8	
      ```

   2. 指定ResourceMnager的master节点信息(修改yarn-site.xml)

      ```shell
      vim yarn-site.xml
      ```

      ```xml
      <!-- 指定YARN的ResourceManager的地址 -->
      <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
      </property>
      <!-- Reducer获取数据的⽅式 -->
      <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
      </property>
      ```

5. ##### 配置历史服务器

   1. 配置mapred-site.xml

      ```xml
      <property>
      <name>mapreduce.jobhistory.address</name>
      <value>master:10020</value>
      </property>
      <!-- 历史服务器web端地址 -->
      <property>
      <name>mapreduce.jobhistory.webapp.address</name>
      <value>master:19888</value>
      </property>
      ```

6. 开启日志汇集功能

   1. 配置yarn-site.xml

      ```xml
      <!-- ⽇志聚集功能使能 -->
      <property>
      <name>yarn.log-aggregation-enable</name>
      <value>true</value>
      </property>
      <!-- ⽇志保留时间设置7天 -->
      <property>
      <name>yarn.log-aggregation.retain-seconds</name>
      <value>604800</value>
      </property>
      
      ```
      
   2. 配置完成的yarn-site.xml

   ```xml
   <configuration>
       <!-- Site specific YARN configuration properties -->
       <!-- 指定YARN的ResourceManager的地址 -->
       <property>
           <name>yarn.resourcemanager.hostname</name>
           <value>master</value>
       </property>
       <!-- Reducer获取数据的⽅式 -->
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
       <!-- ⽇志聚集功能使能 -->
       <property>
           <name>yarn.log-aggregation-enable</name>
           <value>true</value>
       </property>
       <!-- ⽇志保留时间设置7天 -->
       <property>
           <name>yarn.log-aggregation.retain-seconds</name>
           <value>604800</value>
       </property>
       <property>
           <name>yarn.nodemanager.vmem-check-enabled</name>
           <value>false</value>
       </property>
   
       <property>
           <name>yarn.nodemanager.pmem-check-enabled</name>
           <value>false</value>
           <description>是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true</description>
       </property>
       <property>
           <name>mapred.child.java.opts</name>
           <value>-Xmx1024m</value>
       </property>
   </configuration>
   
   ```

   

7. 配置环境变量

   ```shell
   vim /etc/profile
   
   export HADOOP_HOME=/opt/software/hadoop
   export PATH=$PATH:$JAVA_HOME/bin:$KAFKA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   
   
   source /etc/profile
   
   ```

8. 通过启动命令启动hdfs以及yarn

   ```shell
   #第一次启动hdfs需要执行
   hadoop namenode -format
   
   
   ####
   
   start-dfs.sh
   start-yarn.sh	
   ##启动历史服务器
   mr-jobhistory-daemon.sh start historyserver
   ```



#### hive的集群安装

1. 下载Hive软件，并解压缩

   ```shell
   cd /opt
   tar zxvf apache-hive-2.3.7-bin.tar.gz
   mv apache-hive-2.3.7-bin hive
   ```

2. 修改环境变量

   ```shell
   # 在 /etc/profile 文件中增加环境变量
   export HIVE_HOME=/opt/software/hive
   export PATH=$PATH:$HIVE_HOME/bin
   # 执行并生效
   source /etc/profile
   ```

3. 修改hive配置

   ```shell
   vi hive-site.xml
   ```

   ```xml
   <?xml version="1.0" encoding="UTF-8" standalone="no"?>
   <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
   <configuration>
       <!-- hive元数据的存储位置 -->
   	<property>
   		<name>javax.jdo.option.ConnectionURL</name>
   		<value>jdbc:mysql://master:3306/hive?
   		createDatabaseIfNotExist=true&amp;useSSL=false</value>
   		<description>JDBC connect string for a JDBC metastore</description>
   	</property>
   	<!-- 指定驱动程序 -->
   	<property>
   		<name>javax.jdo.option.ConnectionDriverName</name>
   		<value>com.mysql.jdbc.Driver</value>
   		<description>Driver class name for a JDBC metastore</description>
   	</property>
   
   	<!-- 连接数据库的用户名 -->
   	<property>
   		<name>javax.jdo.option.ConnectionUserName</name>
   		<value>hive</value>
   		<description>username to use against metastore database</description>
   	</property>
   
     	<!-- 连接数据库的口令 -->
   	<property>
   		<name>javax.jdo.option.ConnectionPassword</name>
   		<value>12345678</value>
   		<description>password to use against metastore
   		database</description>
   	</property>
   	<property>
     		<!-- 数据默认的存储位置(HDFS) -->
     		<name>hive.metastore.warehouse.dir</name>
     		<value>/user/hive/warehouse</value>
     		<description>location of default database for the warehouse</description>
   	</property>
   	<property>
     		<!-- 在命令行中，显示当前操作的数据库 -->
     		<name>hive.cli.print.current.db</name>
     		<value>true</value>
     		<description>Whether to include the current database in the Hive prompt.</description>
   	</property>
   	<property>
       	<name>hive.server2.thrift.client.user</name>
       	<value>root</value>
       	<description>Username to use against thrift client</description>
     	</property>
     	<property>
       	<name>hive.server2.thrift.client.password</name>
       	<value>12345678</value>
       	<description>Password to use against thrift client</description>
     	</property>
   </configuration>
   ```

   ```shell
   #初始化hive元数据
   schematool -dbType mysql -initSchema
   ```

   

4. hiveServer2配置

   1. 修改集群上的 core-site.xml，增加如下内容

      ```xml
      <!-- HiveServer2 连不上10000；hadoop为安装用户 -->
      <!-- root用户可以代理所有主机上的所有用户 -->
      <property>
      <name>hadoop.proxyuser.root.hosts</name>
      <value>*</value>
      </property>
      <property>
      <name>hadoop.proxyuser.root.groups</name>
      <value>*</value>
      </property>
      <property>
      <name>hadoop.proxyuser.hadoop.hosts</name>
      <value>*</value>
      </property>
      <property>
      <name>hadoop.proxyuser.hadoop.groups</name>
      <value>*</value>
      </property>
      
      ```

   2. 修改 集群上的 hdfs-site.xml，增加以下内容：

      ```xml
      <!-- HiveServer2 连不上10000；启用 webhdfs 服务 -->
      <property>
      <name>dfs.webhdfs.enabled</name>
      <value>true</value>
      </property>
      ```

   3. 远程配置，在三个机器同步hive配置

      ```shell
      nohup hive --service metastore &	
      ```

   4. 修改 slave0 上hive-site.xml。删除配置文件中：MySQL的配置、连接数据库
      的用户名、口令等信息；增加连接metastore的配置：

   ```xml
   <!-- hive metastore 服务地址 -->
   <property>
   <name>hive.metastore.uris</name>
   <value>thrift://master:9083,thrift://slave2:9083</value>
   </property>
   ```

5. 启动hiveserver2的命令

   ```shell
   nohup hiveserver2 &
   ##这样就可以通过idea连接了
   
   ```



#### HBase安装配置

1. 解压habse并修改文件夹名称

   ```shell
   tar -zxvf hbase-1.3.1-bin.tar.gz
   mv hbase-1.3.1 hbase
   ```

   + 配置hbase的环境变量

   ```shell
   export HBASE_HOME=/opt/hbase-1.2.1
   export PATH=$PATH:$HBASE_HOME/bin
   ```

   

2. 修改配置文件，需要把hadoop中的配置core-site.xml 、 hdfs-site.xml拷⻉到hbase安装⽬录下的conf⽂件夹中。这里通过软连接的方式实现

   ```shell
   ln -s $HADOOP_HOME/etc/hadoop/core-site.xml core-site.xml
   ln -s $HADOOP_HOME/etc/hadoop/hdfs-site.xml hdfs-site.xml
   ```

3. 修改conf目录下的文件

   + 修改 hbase-env.sh	

     ```
     #添加java环境变量
     export JAVA_HOME=/opt/java8
     #指定使⽤外部的zk集群
     export HBASE_MANAGES_ZK=FALSE
     ```

   + 修改 hbase-site.xml

     ```xml
     <configuration>
     	<!-- 指定hbase在HDFS上存储的路径 -->
     	<property>
     		<name>hbase.rootdir</name>
     		<value>hdfs://master:9000/hbase</value>
     	</property>
     	<!-- 指定hbase是分布式的 -->
     	
     	<property>
     		<name>hbase.cluster.distributed</name>
     		<value>true</value>
     	</property>
     	<!-- 指定zk的地址，多个⽤“,”分割 -->
     	
     	<property>
     		<name>hbase.zookeeper.quorum</name>
     		<value>master,slave0,slave2</value>
     	</property>
     </configuration>
     ```

   + 修改regionservers文件

     ```
     master
     slave0
     slave2
     ```

4. 启动Hbase

   ```shell
   ## 前提条件：先启动hadoop和zk集群
   ##启动HBase：
   start-hbase.sh
   ##停⽌HBase：
   stop-hbase.sh
   ```

   启动好HBase集群之后，可以访问地址： HMaster的主机名:16010




#### Tez的配置

 1. 解压文件apache-tez-0.9.0-bin.tar.gz

    

    ```shell
    tar -zxvf apache-tez-0.9.2-bin.tar.gz -C /opt/software
    cd /opt/software
    mv apache-tez-0.9.2-bin tez
    ```

	2. 将tez的压缩包放到hdfs上

    ```shell
    cd /opt/software/tez/share
    hdfs dfs -mkdir -p /user/tez
    hdfs dfs -put tez.tar.gz /user/tez
    ```

	3. $HADOOP_HOME/etc/hadoop/ 下创建 tez-site.xml 文件，做如下配置

    ```xml
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
    <!-- 指定在hdfs上的tez包文件 -->
    <property>
    <name>tez.lib.uris</name>
    <value>hdfs://master:9000/user/tez/tez.tar.gz</value>
    </property>
    </configuration>
    ```

    将配置分发到所有节点

	4. 增加客户端节点的配置(/etc/profile)

    ```profile
    export TEZ_CONF_DIR=$HADOOP_CONF_DIR
    export TEZ_JARS=/opt/software/tez/*:/opt/software/tez/lib/*
    export HADOOP_CLASSPATH=$TEZ_CONF_DIR:$TEZ_JARS:$HADOOP_CLASSPATH
    ```

	5. Hive设置TEZ执行

    ```shell
    hive> set hive.execution.engine=tez；
    ```

	6. 如果想默认使用Tez，可在$HIVE_HOME/conf目录下hive-site.xml 中增加

    ```xml
    <property>
    <name>hive.execution.engine</name>
    <value>tez</value>
    </property>
    ```

    

#### Spark安装

1. 下载软件解压缩，移动到指定位置

   ```shell
   cd /opt/software/
   tar -zxvf spark-2.4.5-bin-without-hadoop.tgz -C ../
   mv spark-2.4.5-bin-without-hadoop spark-2.4.5
   
   ```

2. 设置环境变量

   ```shell
   vi /etc/profile
   export SPARK_HOME=/opt/spark-2.4.5
   export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
   source /etc/profile
   ```

3. 修改配置

   文件位置：$SPARK_HOME/conf

   修改文件：slaves、spark-defaults.conf、spark-env.sh、log4j.properties

   Slaves

   ```slave
   master
   slave0
   slave2
   ```

   Spark-defaults.conf

   ```conf
   spark.master spark://master:7077
   spark.  .enabled true
   spark.eventLog.dir hdfs://master:9000/sparkeventlog
   spark.serializer org.apache.spark.serializer.KryoSerializer
   spark.driver.memory 512m
   spark.yarn.jars hdfs:///spark-yarn/jars/*.jar
   spark.yarn.historyServer.address master:18080
   spark.history.ui.port 18080
   ```

   创建HDFS目录：hdfs dfs -mkdir /spark-eventlog

   备注：

   spark.master。定义master节点，缺省端口号 7077
   spark.eventLog.enabled。开启eventLog
   spark.eventLog.dir。eventLog的存放位置
   spark.serializer。一个高效的序列化器
   spark.driver.memory。定义driver内存的大小（缺省1G

   修改spark-env.sh

   ```shell
   export JAVA_HOME=/opt/jdk8
   export HADOOP_HOME=/opt/hadoop-2.9.2
   export HADOOP_CONF_DIR=/opt/hadoop-2.9.2/etc/hadoop
   export SPARK_DIST_CLASSPATH=$(/opt/hadoop-2.9.2/bin/hadoop classpath)
   export SPARK_MASTER_HOST=master
   export SPARK_MASTER_PORT=7077
   ```

   

4. 将Spark软件分发到集群；修改其他节点上的环境变量

   ```shell
   cd /opt/
   scp -r spark-2.4.5/ slave0:$PWD
   scp -r spark-2.4.5/ slave2:$PWD
   ```

   

5. 启动集群

   ```shell
   cd $SPARK_HOME/sbin
   ./start-all.sh
   ```

以上模式为standAlone模式的部署



##### history server



```shell
# spark-defaults.conf
# history server
spark.eventLog.enabled true
spark.eventLog.dir hdfs://master:8020/spark-eventlog
spark.eventLog.compress true
# spark-env.sh
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -
Dspark.history.retainedApplications=50 
-Dspark.history.fs.logDirectory=hdfs://master:9000/sparkeventlog"
```



#### Flink的安装

##### standalone模式安装







#### Kylin安装

##### 	依赖环境

| 软件      | 版本   |
| --------- | ------ |
| Hive      | 2.3.7  |
| Hadoop    | 2.9.2  |
| Hbase     | 1.1.2  |
| Zookeeper | 3.4.14 |
| Kafka     | 1.0.2  |
| Spark     | 2.4.5  |



##### 	kylin的安装配置

1. 解压缩软件

   ```shell
   cd /opt/software
   tar -zxvf apache-kylin-3.1.1-bin-hbase1x.tar.gz -C ../
   mv apache-kylin-3.1.1-bin-hbase1x kylin-3.1.1
   ```

   

2. 添加环境变量

   ```shell
   vi /etc/profile
   # 增加以下内容
   export KYLIN_HOME=/opt/kylin-3.1.1
   export PATH=$PATH:$KYLIN_HOME/bin
   source /etc/profile
   ```

3. 增加kylin依赖组件的配置

   ```shell
   cd $KYLIN_HOME/conf
   ln -s $HADOOP_HOME/etc/hadoop/hdfs-site.xml hdfs-site.xml
   ln -s $HADOOP_HOME/etc/hadoop/core-site.xml core-site.xml
   ln -s $HBASE_HOME/conf/hbase-site.xml hbase-site.xml
   ln -s $HIVE_HOME/conf/hive-site.xml hive-site.xml
   ln -s $SPARK_HOME/conf/spark-defaults.conf spark-defaults.conf
   ```

4. 修改kylin.sh

   ```shell
   cd $KYLIN_HOME/bin
   vim kylin.sh
   # 在 kylin.sh 文件头部添加
   export HADOOP_HOME=/opt/hadoop-2.9.2
   export HIVE_HOME=/opt/hive-2.3.7
   export HBASE_HOME=/opt/hbase-1.1.2
   export SPARK_HOME=/opt/spark-2.4.5
   ```




#### druid的安装

1. ##### 下载druid,并解压

   ```shell
   cd /opt/software
   wget https://mirrors.tuna.tsinghua.edu.cn/apache/druid/0.19.0/apache-druid-0.19.0-bin.tar.gz
   tar -zxvf apache-druid-0.19.0-bin.tar.gz -C ../
   
   cd ../
   mv apache-druid-0.19.0 druid-0.19.0
   ```

   

2. ##### 设置环境变量

   ```shell
   vim /etc/profile
   # 在文件中增加以下内容
   export DRUID_HOME=/opt/druid-0.19.0
   export PATH=$PATH:$DRUID_HOME/bin
   ```

3. ##### MySql中创建响应的数据库

   ```SQL
   CREATE DATABASE druid DEFAULT CHARACTER SET utf8mb4;
   CREATE USER 'druid'@'%' IDENTIFIED BY '12345678';
   GRANT ALL PRIVILEGES ON druid.* TO 'druid'@'%';	
   ```

4. ##### 配置Druid参数

   1. 将hadoop配置文件core-hdfs.xml,hdfs-site.xml,yarn-site.xml,mapred.xml链接到conf/druid/cluster/_common下

      ```shell
      cd $DRUID_HOME/conf/druid/cluster/_common
      ln -s $HADOOP_HOME/etc/hadoop/core-site.xml core-site.xml
      ln -s $HADOOP_HOME/etc/hadoop/hdfs-site.xml hdfs-site.xml
      ln -s $HADOOP_HOME/etc/hadoop/yarn-site.xml yarn-site.xml
      ln -s $HADOOP_HOME/etc/hadoop/mapred-site.xml mapred-site.xml
      ```

      

   2. 配置mysql驱动程序

      ```shell
      ln -s $HIVE_HOME/lib/mysql-connector-java-5.1.46.jar mysql-connector-java-5.1.46.jar
      ```

   3. 修改配置文件（$DRUID_HOME/conf/druid/cluster/_common/common.runtime.properties）

      ```properties
      # 增加"mysql-metadata-storage"
      druid.extensions.loadList=["mysql-metadata-storage", "druid-hdfs-storage", "druid-kafkaindexing-service", "druid-datasketches"]
      # 每台机器写自己的ip或hostname
      druid.host=master
      # 填写zk地址
      druid.zk.service.host=master:2181,salve0:2181,slave2:2181
      druid.zk.paths.base=/druid
      
      # 注释掉前面 derby 的配置
      # 增加 mysql 的配置
      druid.metadata.storage.type=mysql
      druid.metadata.storage.connector.connectURI=jdbc:mysql://slave1:3306/druid
      druid.metadata.storage.connector.user=druid
      druid.metadata.storage.connector.password=12345678
      
      # 注释掉local的配置
      # 增加HDFS的配置，即使用HDFS作为深度存储
      druid.storage.type=hdfs
      druid.storage.storageDirectory=/druid/segments
      
      # 注释掉 indexer.logs For local disk的配置
      # 增加 indexer.logs For HDFS 的配置
      druid.indexer.logs.type=hdfs
      druid.indexer.logs.directory=/druid/indexing-logs
      ```

   4. 配置主节点文件（参数大小根据实际情况配置）

      $DRUID_HOME/conf/druid/cluster/master/coordinator-overlord/jvm.config

      ```config
      -server
      -Xms512m
      -Xmx512m
      -XX:+ExitOnOutOfMemoryError
      -XX:+UseG1GC
      -Duser.timezone=UTC+8
      -Dfile.encoding=UTF-8
      -Djava.io.tmpdir=var/tmp
      -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
      ```

      

   5. 配置数据节点文件(参数大小根据实际情况配置)

      $DRUID_HOME/conf/druid/cluster/data/historical/jvm.config

      ```config
      -server
      -Xms512m
      -Xmx512m
      -XX:MaxDirectMemorySize=1g
      -XX:+ExitOnOutOfMemoryError
      -Duser.timezone=UTC+8
      -Dfile.encoding=UTF-8
      -Djava.io.tmpdir=var/tmp
      -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
      ```

   6. 修改文件$DRUID_HOME/conf/druid/cluster/data/historical/runtime.properties

      ```properties
      # 修改这一个参数
      druid.processing.buffer.sizeBytes=50000000
      ```

      备注：druid.processing.buffer.sizeBytes 每个查询用于聚合的堆外哈希表的大小
      maxDirectMemory= druid.processing.buffer.sizeBytes * (druid.processing.numMergeBuffers +
      druid.processing.numThreads + 1)
      如果 druid.processing.buffer.sizeBytes 太大，那么需要加大maxDirectMemory，否则 historical 服务无法启动

   7. 修改文件$DRUID_HOME/conf/druid/cluster/data/middleManager/jvm.config

      ```config
      -server
      -Xms128m
      -Xmx128m
      -XX:+ExitOnOutOfMemoryError
      -Duser.timezone=UTC+8
      -Dfile.encoding=UTF-8
      -Djava.io.tmpdir=var/tmp
      -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
      ```

   8. 配置查询节点文件（参数大小根据实际情况配置）$DRUID_HOME/conf/druid/cluster/query/broker/jvm.config

      ```config
      -server
      -Xms512m
      -Xmx512m
      -XX:MaxDirectMemorySize=512m
      -XX:+ExitOnOutOfMemoryError
      -Duser.timezone=UTC+8
      -Dfile.encoding=UTF-8
      -Djava.io.tmpdir=var/tmp
      -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
      ```

       $DRUID_HOME/conf/druid/cluster/query/broker/runtime.properties

      ```properties
      # 修改这一个参数
      druid.processing.buffer.sizeBytes=50000000
      ```

      备注：
      druid.processing.buffer.sizeBytes 每个查询用于聚合的堆外哈希表的大小
      maxDirectMemory = druid.processing.buffer.sizeBytes*(druid.processing.numMergeBuffers +
      druid.processing.numThreads + 1)
      如果 druid.processing.buffer.sizeBytes 太大，那么需要加大maxDirectMemory，否则 broker 服务无法启动

      $DRUID_HOME/conf/druid/cluster/query/router/jvm.config

      ```
      -server
      -Xms128m
      -Xmx128m
      -XX:+UseG1GC
      -XX:MaxDirectMemorySize=128m
      -XX:+ExitOnOutOfMemoryError
      -Duser.timezone=UTC+8
      -Dfile.encoding=UTF-8
      -Djava.io.tmpdir=var/tmp
      -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
      ```

      小结：各服务 JVM 内存分配设置如下：
      coordinator-overlord，512m
      historical，512m、堆外1g
      middleManager，128m
      broker，512m、堆外 512m
      router，128m、堆外 128m

5. ##### 分发并启动服务

   1. 向slave0、slave2

      ```shell
      scp -r druid-0.19.0/ slave0:$PWD
      scp -r druid-0.19.0/ slave2:$PWD
      ```

   2. 在master节点启动服务

      ```shell
      nohup start-cluster-master-no-zk-server &
      ```

      